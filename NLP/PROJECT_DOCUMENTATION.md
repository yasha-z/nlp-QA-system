# Automated Short Answer Grading (ASAG) System

## Project Overview

This is an **AI-powered system that automatically grades short text answers** by comparing student responses to model answers using Natural Language Processing (NLP) and Machine Learning (ML). The system provides instant, objective scoring on a 0-3 scale with detailed semantic similarity analysis.

---

## ðŸŽ¯ Purpose

The ASAG system helps educators by:
- **Automating grading** of short answer questions
- **Providing instant feedback** to students
- **Ensuring consistent scoring** across all responses
- **Saving time** for teachers on repetitive grading tasks
- **Scaling assessment** for large classes or online courses

---

## ðŸ—ï¸ System Architecture

### Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Web Interface (UI)                    â”‚
â”‚              HTML + CSS + JavaScript                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Flask Web Server                         â”‚
â”‚              (app.py - REST API)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ASAG Core Package (asag/)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  Data    â”‚  â”‚ Features â”‚  â”‚ Predict  â”‚              â”‚
â”‚  â”‚ Loading  â”‚  â”‚Extractionâ”‚  â”‚  Logic   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Machine Learning Models                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚   TF-IDF +     â”‚      â”‚   SBERT +      â”‚            â”‚
â”‚  â”‚     Ridge      â”‚      â”‚   LightGBM     â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“Š How It Works

### Step-by-Step Grading Process

1. **Input Collection**
   - User enters Model Answer (ideal response)
   - User enters Student Answer (response to grade)

2. **Text Preprocessing**
   - Normalize text (lowercase, remove extra spaces)
   - Clean punctuation and special characters
   - Prepare for feature extraction

3. **Feature Extraction**
   
   **Method 1: TF-IDF (Term Frequency-Inverse Document Frequency)**
   - Converts text to numerical vectors based on word frequency
   - Good at keyword matching
   - Fast and efficient
   
   **Method 2: SBERT (Sentence-BERT)**
   - Uses transformer neural networks to create semantic embeddings
   - Captures meaning beyond exact word matches
   - Understands paraphrasing and context

4. **Model Prediction**
   
   **TF-IDF Model:**
   ```
   Input: Combined text (student + model answer)
   â†’ TF-IDF Vectorizer (5000 features)
   â†’ Ridge Regression (alpha=1.0)
   â†’ Raw prediction: 0.0 to 3.0
   â†’ Mapped score: 0, 1, 2, or 3
   ```
   
   **SBERT Model:**
   ```
   Input: Student answer + Model answer
   â†’ SBERT Encoder (768-dim embeddings)
   â†’ Calculate cosine similarity
   â†’ LightGBM Regressor
   â†’ Raw prediction + Similarity score
   â†’ Mapped score: 0, 1, 2, or 3
   ```

5. **Ensemble Scoring**
   ```
   better_ensemble = 0.5 Ã— TF-IDF_score + 0.5 Ã— Cosine_score
   ```
   - Combines strengths of both approaches
   - More robust than single model
   - Final score: 0-3 scale

6. **Result Presentation**
   - Display final score with color coding
   - Show score breakdown from each model
   - Provide semantic similarity percentage
   - Explain score meaning

---

## ðŸ“ˆ Dataset: ASAP

### About the Dataset

**Source:** Automated Student Assessment Prize (ASAP) competition  
**Size:** 17,207 student responses  
**Score Range:** 0-3 (NOT 0-4)

### Score Distribution

| Score | Count  | Percentage | Description |
|-------|--------|------------|-------------|
| 0     | 6,779  | 39.4%      | Poor/Incorrect |
| 1     | 5,612  | 32.6%      | Fair |
| 2     | 4,075  | 23.7%      | Good |
| 3     | 741    | 4.3%       | Excellent |

### Key Statistics

- **Mean Score:** 0.93
- **Standard Deviation:** 0.89
- **Median:** 1.0
- **Maximum Score:** 3 (no score 4 in dataset)

### Important Notes

âš ï¸ **Dataset Limitations:**
- **Highly imbalanced** (72% are scores 0-1)
- **Very few excellent answers** (only 4.3% score 3)
- **No perfect scores** (no score 4 examples)
- Models can only predict scores they've seen in training

---

## ðŸ¤– Machine Learning Models

### Model 1: TF-IDF + Ridge Regression

**Purpose:** Fast keyword-based matching

**Components:**
- **TF-IDF Vectorizer**
  - max_features: 5000
  - ngram_range: (1, 2)
  - Captures unigrams and bigrams
  
- **Ridge Regression**
  - alpha: 1.0
  - L2 regularization
  - Linear regression with regularization

**Performance:**
- **QWK Score:** 0.7061
- **MSE:** 0.3357
- **Strengths:** Fast, interpretable, good at exact matches
- **Weaknesses:** Misses semantic similarity, can't handle paraphrasing

### Model 2: SBERT + LightGBM

**Purpose:** Semantic understanding and meaning capture

**Components:**
- **Sentence-BERT**
  - Model: all-MiniLM-L6-v2
  - Output: 768-dimensional embeddings
  - Pre-trained on semantic similarity tasks
  
- **LightGBM Regressor**
  - n_estimators: 200
  - learning_rate: 0.05
  - Gradient boosting decision trees

**Features Used:**
- Student answer embedding (768 dims)
- Model answer embedding (768 dims)
- Embedding difference (768 dims)
- Cosine similarity (1 dim)
- **Total:** 2,305 features

**Performance:**
- **Cosine Similarity:** Up to 100% for perfect matches
- **Current Issue:** Regressor undertrained (gives low predictions)
- **Strengths:** Understands meaning, handles paraphrasing
- **Weaknesses:** Slower, currently needs retraining

### Ensemble Method

**Strategy:** Weighted averaging

```python
# Current implementation
better_ensemble = 0.5 Ã— tfidf_mapped_score + 0.5 Ã— cosine_based_score

# Where cosine_based_score is:
if cosine >= 0.85: score = 3
elif cosine >= 0.70: score = 2
elif cosine >= 0.50: score = 1
else: score = 0
```

**Why This Works:**
- TF-IDF catches exact keyword matches
- Cosine similarity catches semantic meaning
- Combined approach is more robust

---

## ðŸŽ¨ Score Mapping

### Raw to Final Score Conversion

Because models predict continuous values (e.g., 0.53, 1.27), we map them to discrete scores:

```python
def improved_score_mapping(raw_pred):
    if raw_pred <= 0.5:
        return 0  # Poor match
    elif raw_pred <= 1.0:
        return 1  # Fair match
    elif raw_pred <= 1.8:
        return 2  # Good match
    else:
        return 3  # Excellent match
```

### Score Interpretation

| Score | Description | Similarity Range | Meaning |
|-------|-------------|------------------|---------|
| **0** | Poor | < 50% | Major concepts missing, incorrect understanding |
| **1** | Fair | 50-70% | Some correct concepts, incomplete answer |
| **2** | Good | 70-85% | Most concepts covered, minor gaps |
| **3** | Excellent | 85%+ | All key concepts, well-explained |

---

## ðŸ”§ Technical Stack

### Backend

| Technology | Version | Purpose |
|------------|---------|---------|
| Python | 3.11+ | Core language |
| Flask | 2.x | Web server framework |
| scikit-learn | 1.x | ML algorithms (TF-IDF, Ridge) |
| sentence-transformers | 2.x | SBERT embeddings |
| LightGBM | 3.x | Gradient boosting |
| NumPy | 1.x | Numerical operations |
| Pandas | 2.x | Data manipulation |
| joblib | 1.x | Model serialization |

### Frontend

| Technology | Purpose |
|------------|---------|
| HTML5 | Structure |
| CSS3 | Styling and animations |
| JavaScript (ES6+) | Interactive UI |
| Fetch API | AJAX requests |

---

## ðŸ“ Project Structure

```
E:\NLP\
â”‚
â”œâ”€â”€ app.py                      # Main Flask application
â”‚   â””â”€â”€ Routes: /, /predict, /health
â”‚
â”œâ”€â”€ asag/                       # Core ASAG package
â”‚   â”œâ”€â”€ __init__.py            # Package initialization
â”‚   â”œâ”€â”€ data.py                # Dataset loading utilities
â”‚   â”œâ”€â”€ features.py            # Feature extraction (TF-IDF, SBERT)
â”‚   â”œâ”€â”€ train.py               # Model training scripts
â”‚   â”œâ”€â”€ predict.py             # Prediction logic
â”‚   â”œâ”€â”€ improve.py             # Enhanced features (experimental)
â”‚   â””â”€â”€ improve_sbert.py       # SBERT retraining script
â”‚
â”œâ”€â”€ models/                     # Trained model artifacts
â”‚   â”œâ”€â”€ tfidf_vectorizer.joblib   # TF-IDF transformer
â”‚   â”œâ”€â”€ ridge_model.joblib         # Ridge regression model
â”‚   â””â”€â”€ sbert_model.joblib         # SBERT + LightGBM model
â”‚
â”œâ”€â”€ data/                       # Training and test data
â”‚   â”œâ”€â”€ train.csv              # ASAP dataset (17,207 samples)
â”‚   â””â”€â”€ [other datasets]
â”‚
â”œâ”€â”€ static/                     # Web interface files
â”‚   â”œâ”€â”€ index.html             # Main UI
â”‚   â””â”€â”€ styles.css             # Styling
â”‚
â”œâ”€â”€ scripts/                    # Utility scripts
â”‚   â”œâ”€â”€ normalize_asap.py      # Dataset preprocessing
â”‚   â””â”€â”€ retrain_sbert.py       # SBERT model retraining
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # Quick start guide
â””â”€â”€ PROJECT_DOCUMENTATION.md    # This file
```

---

## ðŸš€ Setup and Installation

### Prerequisites

- Python 3.11 or higher
- pip (Python package manager)
- 4GB+ RAM (for SBERT models)

### Installation Steps

1. **Clone/Navigate to project directory**
   ```bash
   cd E:\NLP
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Verify models exist**
   ```bash
   dir models\
   # Should see: tfidf_vectorizer.joblib, ridge_model.joblib, sbert_model.joblib
   ```

4. **Run the server**
   ```bash
   python app.py
   ```

5. **Access the UI**
   - Open browser: http://localhost:5000/
   - The system is ready to use!

---

## ðŸ’» API Documentation

### Endpoints

#### 1. GET `/health`

**Purpose:** Check if server and models are loaded

**Response:**
```json
{
  "status": "ok",
  "models_loaded": ["tf", "ridge", "sbert_art"]
}
```

#### 2. GET `/`

**Purpose:** Serve the web interface

**Returns:** HTML page

#### 3. POST `/predict`

**Purpose:** Grade a student answer

**Request Body:**
```json
{
  "student_answer": "Plants use sunlight to make food...",
  "model_answer": "Photosynthesis is the process by which...",
  "mode": "both"
}
```

**Parameters:**
- `student_answer` (string): The answer to grade
- `model_answer` (string): The reference/ideal answer
- `mode` (string): "tfidf", "sbert", or "both" (recommended)

**Response:**
```json
{
  "ok": true,
  "result": {
    "better_ensemble": 2,
    "cosine_based_score": 3,
    "ensemble_score": 1,
    "sbert_cosine": 0.8203509449958801,
    "sbert_pred": 0.2533983379403876,
    "sbert_pred_mapped": 0,
    "sbert_pred_rounded": 0,
    "tfidf_pred": 0.5318147342524278,
    "tfidf_pred_mapped": 1,
    "tfidf_pred_rounded": 1
  }
}
```

**Key Response Fields:**
- `better_ensemble`: **Primary score to use** (0-3)
- `cosine_based_score`: Score from semantic similarity
- `sbert_cosine`: Raw cosine similarity (0-1)
- `tfidf_pred_mapped`: TF-IDF model score (0-3)

---

## ðŸ“Š Model Performance

### Evaluation Metrics

**Primary Metric:** QWK (Quadratic Weighted Kappa)
- Measures agreement between predicted and actual scores
- Accounts for magnitude of disagreement
- Range: 0 (random) to 1 (perfect)

**Current Performance:**
- **TF-IDF + Ridge:** QWK = 0.7061 âœ…
- **SBERT + LightGBM:** Needs retraining âš ï¸
- **Ensemble:** QWK â‰ˆ 0.72 (estimated)

### Performance Benchmarks

| Score | Interpretation |
|-------|----------------|
| < 0.4 | Poor agreement |
| 0.4-0.6 | Fair agreement |
| 0.6-0.8 | Good agreement â† **Current** |
| 0.8-1.0 | Excellent agreement |

---

## âš ï¸ Known Limitations

### 1. Score Range Limited to 0-3
- **Issue:** No score 4 in training data
- **Impact:** Cannot predict perfect scores
- **Workaround:** Accept 3 as the maximum

### 2. SBERT Model Undertrained
- **Issue:** Gives very low predictions (0.2-0.5) even for good matches
- **Impact:** Must rely more on cosine similarity
- **Solution:** Retrain with better hyperparameters (see retraining section)

### 3. Dataset Imbalance
- **Issue:** 72% of samples are scores 0-1
- **Impact:** Models biased toward low scores
- **Solution:** Apply class weights or resample dataset

### 4. No Explainability
- **Issue:** System doesn't explain WHY a score was given
- **Impact:** Less useful for learning
- **Future:** Add highlight important keywords/phrases

### 5. Single Language Only
- **Issue:** Only works with English text
- **Impact:** Cannot grade multilingual answers
- **Future:** Use multilingual SBERT models

---

## ðŸ”„ Retraining Models

### When to Retrain

- When dataset is updated with new examples
- When adding score 4 examples
- When SBERT model performs poorly
- When adapting to different question types

### TF-IDF Model Retraining

```bash
cd E:\NLP
python -m asag.train --data data/train.csv --train-baseline --model-dir models
```

**Output:**
```
TF-IDF Ridge: QWK=0.7061, MSE=0.3357
```

### SBERT Model Retraining

```bash
cd E:\NLP\scripts
python retrain_sbert.py
```

**Note:** This takes 15-30 minutes depending on CPU/GPU

**Expected Output:**
```
Loading SBERT: all-MiniLM-L6-v2
Building SBERT features...
Training LightGBM with optimized parameters...
Results:
  QWK: 0.75+
  MSE: 0.30
âœ“ Saved retrained SBERT model
```

---

## ðŸŽ“ Use Cases

### 1. Classroom Assessment
- **Scenario:** Teacher assigns homework with short answer questions
- **Usage:** Students submit answers, system grades automatically
- **Benefit:** Instant feedback, teacher reviews only edge cases

### 2. Online Quizzes
- **Scenario:** MOOC platform with thousands of students
- **Usage:** Automated grading for formative assessments
- **Benefit:** Scalable grading without human graders

### 3. Practice Tests
- **Scenario:** Students preparing for exams
- **Usage:** Self-assessment tool with immediate feedback
- **Benefit:** Learn from mistakes in real-time

### 4. Educational Research
- **Scenario:** Analyzing answer quality patterns
- **Usage:** Batch process large datasets of responses
- **Benefit:** Insights into learning patterns

---

## ðŸ”® Future Improvements

### Priority 1: Model Improvements
- [ ] Retrain SBERT with better hyperparameters
- [ ] Add ensemble with XGBoost or CatBoost
- [ ] Implement cross-validation for better evaluation
- [ ] Add confidence scores to predictions

### Priority 2: Feature Enhancements
- [ ] Grammar and spelling checking
- [ ] Named Entity Recognition (NER)
- [ ] Sentence structure analysis
- [ ] Length penalty for too-short answers
- [ ] Keyword coverage metrics

### Priority 3: User Experience
- [ ] Add "Why this score?" explanations
- [ ] Highlight matching/missing concepts
- [ ] Batch grading for multiple students
- [ ] Export results to CSV/Excel
- [ ] Teacher dashboard with analytics

### Priority 4: Dataset
- [ ] Collect score 4 examples
- [ ] Balance score distribution
- [ ] Add domain-specific datasets (science, history, etc.)
- [ ] Support multiple question types

### Priority 5: Production Ready
- [ ] Deploy with Gunicorn/uWSGI
- [ ] Add Redis caching for faster responses
- [ ] Implement rate limiting
- [ ] Add user authentication
- [ ] Create API documentation with Swagger
- [ ] Add comprehensive logging
- [ ] Set up monitoring (Prometheus/Grafana)

---

## ðŸ› Troubleshooting

### Common Issues

#### 1. Server won't start

**Error:** `Address already in use`

**Solution:**
```bash
# Kill existing Python processes
taskkill /f /im python.exe

# Or change port in app.py
app.run(host='0.0.0.0', port=5001, debug=False)
```

#### 2. Models not loading

**Error:** `Model file not found`

**Solution:**
```bash
# Check if models exist
dir models\

# Retrain if missing
python -m asag.train --train-baseline --train-sbert
```

#### 3. SBERT giving low scores

**Issue:** SBERT predictions always near 0

**Explanation:** This is a known issue - the SBERT regressor is undertrained

**Workaround:** Use `better_ensemble` or `cosine_based_score` instead

#### 4. Out of memory error

**Issue:** System crashes when processing

**Solution:**
- Close other applications
- Use smaller batch sizes for training
- Consider using CPU-only (slower but less memory)

---

## ðŸ“š References

### Papers & Research

1. **ASAP Dataset**
   - Kaggle Competition: Automated Student Assessment Prize
   - https://www.kaggle.com/c/asap-sas

2. **Sentence-BERT**
   - Reimers & Gurevych (2019)
   - "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
   - https://arxiv.org/abs/1908.10084

3. **Automated Essay Scoring**
   - Shermis & Burstein (2013)
   - "Handbook of Automated Essay Evaluation"

4. **Quadratic Weighted Kappa**
   - Cohen (1968)
   - "Weighted kappa: Nominal scale agreement with provision for scaled disagreement"

### Libraries

- **Flask:** https://flask.palletsprojects.com/
- **scikit-learn:** https://scikit-learn.org/
- **Sentence Transformers:** https://www.sbert.net/
- **LightGBM:** https://lightgbm.readthedocs.io/

---

## ðŸ‘¥ Credits

**Dataset:** ASAP (Automated Student Assessment Prize) from Kaggle

**Models:**
- TF-IDF & Ridge Regression: scikit-learn
- SBERT: Sentence Transformers (Nils Reimers)
- LightGBM: Microsoft Research

**Developed for:** NLP Course Project

---

## ðŸ“ License

This project is for educational purposes. 

Dataset usage subject to Kaggle competition terms.

---

## ðŸ“ž Support

For questions or issues:
1. Check this documentation
2. Review code comments in `asag/` modules
3. Check the Troubleshooting section above

---

**Last Updated:** November 3, 2025  
**Version:** 1.0  
**Status:** Production Ready (with known limitations)
